[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Mark, a data scientist."
  },
  {
    "objectID": "posts/2-binning.html",
    "href": "posts/2-binning.html",
    "title": "Binning With Tree",
    "section": "",
    "text": "Code\nimport polars as pl\nimport plotly.express as px\n\nimport plotly.io as pio\nimport matplotlib.pyplot as plt\n\npio.renderers.default = \"notebook\"\n\n\n\n\nCode\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/2-binning.html#load-data",
    "href": "posts/2-binning.html#load-data",
    "title": "Binning With Tree",
    "section": "",
    "text": "Code\nimport polars as pl\nimport plotly.express as px\n\nimport plotly.io as pio\nimport matplotlib.pyplot as plt\n\npio.renderers.default = \"notebook\"\n\n\n\n\nCode\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/2-binning.html#data-preparation",
    "href": "posts/2-binning.html#data-preparation",
    "title": "Binning With Tree",
    "section": "2 Data Preparation",
    "text": "2 Data Preparation\n\n\nCode\ndf = pl.read_csv(\"data/credit.csv.gz\")\n\n\n\n\nCode\nlayout_config = dict(\n    width=800,\n    height=500,\n    template=\"simple_white\",\n    plot_bgcolor=\"white\",\n    paper_bgcolor=\"white\",\n    # xaxis=dict(\n    #     showline=True,\n    #     linewidth=1,\n    #     linecolor=\"black\",\n    #     mirror=True,\n    #     showgrid=True,\n    # ),\n    # yaxis=dict(\n    #     showline=True,\n    #     linewidth=1,\n    #     linecolor=\"black\",\n    #     mirror=True,\n    #     showgrid=True,\n    # ),\n)\n\nfig = df.pipe(px.histogram, x=\"limit_bal\", nbins=40, color=\"y\", histnorm=\"density\")\nfig.update_layout(\n    title=dict(text=\"Distribution of Balance\", x=0.5, xanchor=\"center\"), **layout_config\n)\nfig.update_layout(bargap=0.3, xaxis_tickformat=\",d\")\nfig.show()\n\n\n                                                \n\n\nHow to evaluate the performance of the binning?"
  },
  {
    "objectID": "posts/2-binning.html#binning-and-credit-modelling",
    "href": "posts/2-binning.html#binning-and-credit-modelling",
    "title": "Binning With Tree",
    "section": "3 Binning and Credit Modelling",
    "text": "3 Binning and Credit Modelling\nBinning is required to for WOE transformation, IV and PSI calculation. It has many advantages - Robustness (against outliers)\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\ndf_train, df_valid = train_test_split(df, test_size=0.5, random_state=42)\n\n\n\n3.1 Calculate WOE using QCUT\nMore or less bins? - More bins - Less information lost, less bias - Less bins - More stable\nThe easiest cut is qcut\n\n\nCode\ndef calculate_woe(df, x, y):\n    df_woe = (\n        df.group_by(x)\n        .agg(\n            pl.len().alias(\"obs\"),\n            pl.col(y).eq(0).sum().alias(\"y=0\"),\n            pl.col(y).sum().alias(\"y=1\"),\n        )\n        .with_columns(\n            (pl.col(\"y=0\", \"y=1\") / pl.col(\"y=0\", \"y=1\").sum()).name.suffix(\"_rate\"),\n        )\n        .with_columns(\n            (pl.col(\"y=1_rate\") / pl.col(\"y=0_rate\")).log().alias(\"woe\"),\n        )\n        .with_columns(\n            (pl.col(\"y=1_rate\") - pl.col(\"y=0_rate\")).mul(pl.col(\"woe\")).alias(\"iv\")\n        )\n        .pipe(lambda x: x.sort(x.columns[0]))\n    )\n\n    return df_woe\n\n\n5 Bins\n\n\nCode\nfrom plotly import graph_objects as go\nfrom plotly.subplots import make_subplots\n\nsubplot_titles = [\n    \"WoE(5 bins)\",\n    \"WoE(10 bins)\",\n    \"WoE(20 bins)\",\n    \"Obs(5 bins)\",\n    \"Obs(10 bins)\",\n    \"Obs(20 bins)\",\n]\n\nfig = make_subplots(\n    rows=2,\n    cols=3,\n    subplot_titles=subplot_titles,\n    horizontal_spacing=0.05,\n    vertical_spacing=0.15,\n    shared_yaxes=True,\n    shared_xaxes=True,\n)\n\nfor i, n_bins in enumerate([5, 10, 20], start=1):\n    df_woe_limit_bal = df_train.pipe(\n        calculate_woe,\n        pl.col(\"limit_bal\").truediv(1_000).qcut(n_bins, allow_duplicates=True),\n        \"y\",\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines+markers\",\n            x=df_woe_limit_bal[\"limit_bal\"],\n            y=df_woe_limit_bal[\"woe\"],\n            name=f\"WoE({n_bins} bins)\",\n        ),\n        row=1,\n        col=i,\n    )\n\n    # Observation count plots in second row\n    fig.add_trace(\n        go.Bar(\n            x=df_woe_limit_bal[\"limit_bal\"],\n            y=df_woe_limit_bal[\"obs\"],\n            name=f\"Obs({n_bins} bins)\",\n            opacity=0.5,\n        ),\n        row=2,\n        col=i,\n    )\n\nfig.update_layout(\n    template=\"simple_white\",\n    height=500,\n    width=900,\n    showlegend=False,\n    title=dict(\n        text=\"WoE and Observation Counts with different number of bins\",\n        x=0.5,\n        xanchor=\"center\",\n    ),\n)\n\nfig.update_xaxes(tickangle=90)\n\n\nfig.show()\n\n\n                                                \n\n\nBias-variance trade off\nMore bins - Less bias - More variance - Less monotonic - Less stable (PSI)\n\n\nCode\ndef get_qcut_breaks(series: pl.Series, q: int = 5):\n    s = (\n        df_train[\"limit_bal\"]\n        .qcut(q, allow_duplicates=True, include_breaks=True)\n        .struct.field(\"breakpoint\")\n        .unique()\n    )\n\n    breaks = s.filter(~s.is_infinite()).sort(descending=False).to_list()\n    return breaks\n\n\n\n\nCode\ndef calculate_psi(expected: pl.Series, actual: pl.Series) -&gt; float:\n\n    expected_dist = expected.rename(\"x\").value_counts().rename({\"count\": \"count_exp\"})\n    actual_dist = actual.rename(\"x\").value_counts().rename({\"count\": \"count_act\"})\n\n    psi = (\n        expected_dist.join(actual_dist, on=\"x\", how=\"full\", coalesce=True)\n        .with_columns(\n            (pl.col(\"count_exp\") / pl.col(\"count_exp\").sum())\n            .fill_null(0.0001)\n            .alias(\"pct_exp\"),\n            (pl.col(\"count_act\") / pl.col(\"count_act\").sum())\n            .fill_null(0.0001)\n            .alias(\"pct_act\"),\n        )\n        .with_columns(\n            psi=(\n                (pl.col(\"pct_act\") - pl.col(\"pct_exp\"))\n                * (pl.col(\"pct_act\") / pl.col(\"pct_exp\")).log()\n            )\n        )\n    )\n\n    return psi\n\n\n\n\nCode\nbreaks = get_qcut_breaks(df_train[\"limit_bal\"], q=10)\n\n\n\n\nCode\nresult = []\nfor q in range(2, 30):\n    breaks = get_qcut_breaks(df_train[\"limit_bal\"], q=q)\n    iv = calculate_woe(df_train, pl.col(\"limit_bal\").cut(breaks), \"y\")\n    psi = calculate_psi(\n        df_train[\"limit_bal\"].cut(breaks), df_valid[\"limit_bal\"].cut(breaks)\n    )\n    result.append(dict(q=q, iv=iv[\"iv\"].sum(), psi=psi[\"psi\"].sum()))\n\n\n\n\nCode\npl.DataFrame(result).head(10)\n\n\n\nshape: (10, 3)\n\n\n\nq\niv\npsi\n\n\ni64\nf64\nf64\n\n\n\n\n2\n0.12329\n0.000028\n\n\n3\n0.138538\n0.00007\n\n\n4\n0.162616\n0.000181\n\n\n5\n0.156567\n0.00018\n\n\n6\n0.164844\n0.000429\n\n\n7\n0.165888\n0.00086\n\n\n8\n0.17931\n0.000432\n\n\n9\n0.182535\n0.000539\n\n\n10\n0.186456\n0.000293\n\n\n11\n0.184024\n0.000561\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n\n\n\nCode\nDecisionTreeClassifier?\n\n\nInit signature:\nDecisionTreeClassifier(\n    *,\n    criterion='gini',\n    splitter='best',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    class_weight=None,\n    ccp_alpha=0.0,\n    monotonic_cst=None,\n)\nDocstring:     \nA decision tree classifier.\n\nRead more in the :ref:`User Guide &lt;tree&gt;`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at\n          each split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features &lt; n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary &lt;random_state&gt;` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes &gt; 2`),\n      - multioutput classifications (i.e. when `n_outputs_ &gt; 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeRegressor : A decision tree regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe :meth:`predict` method operates using the :func:`numpy.argmax`\nfunction on the outputs of :meth:`predict_proba`. This means that in\ncase the highest predicted probabilities are tied, the classifier will\npredict the tied class with the lowest index in :term:`classes_`.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.model_selection import cross_val_score\n&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n&gt;&gt;&gt; clf = DecisionTreeClassifier(random_state=0)\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; cross_val_score(clf, iris.data, iris.target, cv=10)\n...                             # doctest: +SKIP\n...\narray([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\nFile:           ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/sklearn/tree/_classes.py\nType:           ABCMeta\nSubclasses:     ExtraTreeClassifier\n\n\n\n\nCode\nmin_impurity_decrease\n\n\n\n\nCode\nDecisionTreeClassifier?\n\n\nInit signature:\nDecisionTreeClassifier(\n    *,\n    criterion='gini',\n    splitter='best',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    class_weight=None,\n    ccp_alpha=0.0,\n    monotonic_cst=None,\n)\nDocstring:     \nA decision tree classifier.\n\nRead more in the :ref:`User Guide &lt;tree&gt;`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at\n          each split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features &lt; n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary &lt;random_state&gt;` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes &gt; 2`),\n      - multioutput classifications (i.e. when `n_outputs_ &gt; 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeRegressor : A decision tree regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe :meth:`predict` method operates using the :func:`numpy.argmax`\nfunction on the outputs of :meth:`predict_proba`. This means that in\ncase the highest predicted probabilities are tied, the classifier will\npredict the tied class with the lowest index in :term:`classes_`.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.model_selection import cross_val_score\n&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n&gt;&gt;&gt; clf = DecisionTreeClassifier(random_state=0)\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; cross_val_score(clf, iris.data, iris.target, cv=10)\n...                             # doctest: +SKIP\n...\narray([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\nFile:           ~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/sklearn/tree/_classes.py\nType:           ABCMeta\nSubclasses:     ExtraTreeClassifier\n\n\n\n\nCode\ndt = DecisionTreeClassifier(\n    criterion=\"entropy\", min_samples_leaf=0.05, monotonic_cst=[-1]\n)\n\n\n\n\nCode\ndt.fit(df_train[[\"limit_bal\"]], df_train[\"y\"])\n\n\nDecisionTreeClassifier(criterion='entropy', min_samples_leaf=0.05,\n                       monotonic_cst=[-1])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy', min_samples_leaf=0.05,\n                       monotonic_cst=[-1]) \n\n\n\n\nCode\nbreaks = dt.tree_.threshold[dt.tree_.feature &gt;= 0]\n\n\n\n\nCode\ndf_train.group_by(pl.col(\"limit_bal\").cut(breaks)).agg(pl.col(\"y\").mean()).pipe(\n    px.line, x=\"limit_bal\", y=\"y\"\n)\n\n\n                                                \n\n\n\n\nCode\nplt.figure(figsize=(20, 10))\n_ = plot_tree(dt, feature_names=[\"limit_bal\"], proportion=True, fontsize=8)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.select(pl.col(\"limit_bal\"))\n\n\n\nshape: (30_000, 24)\n\n\n\nlimit_bal\nsex\neducation\nmarriage\nage\npay_0\npay_2\npay_3\npay_4\npay_5\npay_6\nbill_amt1\nbill_amt2\nbill_amt3\nbill_amt4\nbill_amt5\nbill_amt6\npay_amt1\npay_amt2\npay_amt3\npay_amt4\npay_amt5\npay_amt6\ny\n\n\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\n20000\n2\n2\n1\n24\n2\n2\n-1\n-1\n-2\n-2\n3913\n3102\n689\n0\n0\n0\n0\n689\n0\n0\n0\n0\n1\n\n\n120000\n2\n2\n2\n26\n-1\n2\n0\n0\n0\n2\n2682\n1725\n2682\n3272\n3455\n3261\n0\n1000\n1000\n1000\n0\n2000\n1\n\n\n90000\n2\n2\n2\n34\n0\n0\n0\n0\n0\n0\n29239\n14027\n13559\n14331\n14948\n15549\n1518\n1500\n1000\n1000\n1000\n5000\n0\n\n\n50000\n2\n2\n1\n37\n0\n0\n0\n0\n0\n0\n46990\n48233\n49291\n28314\n28959\n29547\n2000\n2019\n1200\n1100\n1069\n1000\n0\n\n\n50000\n1\n2\n1\n57\n-1\n0\n-1\n0\n0\n0\n8617\n5670\n35835\n20940\n19146\n19131\n2000\n36681\n10000\n9000\n689\n679\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n220000\n1\n3\n1\n39\n0\n0\n0\n0\n0\n0\n188948\n192815\n208365\n88004\n31237\n15980\n8500\n20000\n5003\n3047\n5000\n1000\n0\n\n\n150000\n1\n3\n2\n43\n-1\n-1\n-1\n-1\n0\n0\n1683\n1828\n3502\n8979\n5190\n0\n1837\n3526\n8998\n129\n0\n0\n0\n\n\n30000\n1\n2\n2\n37\n4\n3\n2\n-1\n0\n0\n3565\n3356\n2758\n20878\n20582\n19357\n0\n0\n22000\n4200\n2000\n3100\n1\n\n\n80000\n1\n3\n1\n41\n1\n-1\n0\n0\n0\n-1\n-1645\n78379\n76304\n52774\n11855\n48944\n85900\n3409\n1178\n1926\n52964\n1804\n1\n\n\n50000\n1\n2\n1\n46\n0\n0\n0\n0\n0\n0\n47929\n48905\n49764\n36535\n32428\n15313\n2078\n1800\n1430\n1000\n1000\n1000\n1\n\n\n\n\n\n\n\n\nCode\nlimit_bal"
  },
  {
    "objectID": "posts/1-tree.html",
    "href": "posts/1-tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "In this note, we explore the key parameters of scikit-learn’s decision tree classifier (DecisionTreeClassifier).\nWe’ll examine each parameter’s purpose, effect on model behavior, and demonstrate with practical examples using toy datasets."
  },
  {
    "objectID": "posts/1-tree.html#introduction",
    "href": "posts/1-tree.html#introduction",
    "title": "Decision Tree",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "posts/1-tree.html#how-to",
    "href": "posts/1-tree.html#how-to",
    "title": "Decision Tree",
    "section": "How to",
    "text": "How to\n\nCode Snippet\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndata = load_breast_cancer()\n\nX, y = data[\"data\"], data[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\ndt = DecisionTreeClassifier(\n    criterion=\"gini\", max_depth=3, min_samples_split=30, min_samples_leaf=15\n)\ndt.fit(X_train, y_train)\n\nDecisionTreeClassifier(max_depth=3, min_samples_leaf=15, min_samples_split=30)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3, min_samples_leaf=15, min_samples_split=30) \n\n\n\n\nVisualisation\n\n\nCode\nplt.figure(figsize=(8, 4))\n_ = plot_tree(dt, feature_names=data[\"feature_names\"], fontsize=7)"
  },
  {
    "objectID": "posts/1-tree.html#behind-the-scene",
    "href": "posts/1-tree.html#behind-the-scene",
    "title": "Decision Tree",
    "section": "Behind the Scene",
    "text": "Behind the Scene\n\nImpurity\nTo understand the parameters in a decision tree classifier, let’s explore how decision trees work under the hood.\nA decision tree works by recursively splitting the data into two groups. At each split, it aims to create “purer” subsets - where one group has a higher concentration of the positive class (1s) and the other has more of the negative class (0s) compared to before the split.\nLet’s look at a simple example: - Starting data: 100 samples total, evenly split with 50 class 0 and 50 class 1 - After splitting: - Left group: 40 samples, with 30 class 0 (75%) and 10 class 1 (25%) - Right group: 60 samples, with 20 class 0 (33%) and 40 class 1 (67%)\nAs we can see, both groups are now “purer” than the original 50-50 split. The left group is dominated by class 0 while the right group has more class 1 samples.\nTo quantify this purity, we use impurity measures that reach their maximum when classes are evenly split (50-50) and their minimum when one class completely dominates (100-0 or 0-100).\nThe two most common impurity measures are gini impurity and entropy, which we’ll explore next.\nGini \\[\\begin{array}{rcl}\n\\text{gini} &=& p_0(1-p_0) + p_1(1-p_1) \\\\\n&=& 2p_0(1-p_0)\\\\\n&=& 1- p_0^2  - p_1^2\n\\end{array}\\]\nEntropy \\[\\text{Entropy} = p_0\\ln p_0 + p_1\\ln p_1\\]\nSee Figure 1. Note that both measures are symmetric, achieving maximum at \\(p_0=0.5\\) (most impure) and minimum at \\(p_0=1\\) and \\(p_0=1\\) (completely pure).\nThis makes intuitive sense: a 50-50 split between classes represents maximum uncertainty, while having all samples from one class represents perfect purity.\n\n\nCode\np0 = np.linspace(0.01, 0.99, 100)\ngini = 2 * p0 * (1 - p0)\nentropy = -p0 * np.log(p0) - (1 - p0) * np.log(1 - p0)\n\nplt.figure(figsize=(6, 4.5))\nplt.plot(p0, entropy, label=\"Entropy\")\nplt.plot(p0, gini, label=\"Gini\")\nplt.legend()\nplt.xlabel(\"$p_0$ (proportion of class 0)\")\nplt.title(\"Gini and Entropy\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Gini and Entropy\n\n\n\n\n\nCalculating Impurity\nLet’s calculate Gini impurity for a concrete example.\nBefore split, with equal class proportions: \\[2\\times0.5\\times0.5 = 0.5\\]\nAfter split into two nodes: \\[\\text{Left node}: 2\\times0.75\\times 0.25 = 0.375\\] \\[\\text{Right node}: 2\\times\\frac{1}{3}\\times \\frac{2}{3} = \\frac{4}{9} \\approx 0.444\\]\nThe split has resulted in two purer nodes compared to the original node, as both impurity values (0.375 and 0.444) are lower than the initial 0.5.\nTo quantify the overall improvement, we calculate the weighted average impurity after the split, where weights are proportional to the number of samples in each node:\n\\[I_{\\text{split}} = \\frac{n_{\\text{left}}}{n}I_{\\text{left}} + \\frac{n_{\\text{right}}}{n}I_{\\text{right}}\\]\nThe reduction in impurity (information gain) is then: \\[\\text{Gain} = I_{\\text{before}} - I_{\\text{split}}\\]\n\n\nFind the Optimal Split\nConsider the following example dataset:\n\n\n\nid\nx\ny\n\n\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n5\n1\n\n\n5\n5\n0\n\n\n6\n7\n0\n\n\n7\n7\n1\n\n\n8\n8\n0\n\n\n9\n8\n1\n\n\n10\n9\n1\n\n\n11\n10\n1\n\n\n12\n11\n0\n\n\n13\n12\n1\n\n\n\nTo find the optimal split point, we need to:\n\nConsider all possible splits of the form \\(x \\leq \\text{cutoff}\\)\nFor each split:\n\nCalculate impurity before split\nCalculate weighted impurity after split\nCompute impurity reduction (information gain)\n\nChoose the split that maximizes information gain\n\nWith 13 data points and 10 unique x-values (2,3,4,5,7,8,9,10,11,12), we have 9 possible split points to evaluate.\nAfter finding the best split on a single feature, we can continue recursively splitting the resulting nodes to build a complete decision tree. At each step, we:\n\nConsider all available features\nFor each feature, find the optimal split point using the process above\nChoose the feature and split point combination that gives the highest information gain\nSplit the node and repeat the process on the child nodes\n\nWe continue splitting until some stopping criterion is met, such as: - Maximum tree depth reached - Minimum number of samples in a node - No further improvement in impurity possible\n\n\nStopping Criteria\n\nmax_depth\nmin_samples_split\nmin_samples_leaf\nmin_weight_fraction_leaf\nmax_leaf_nodes\nmin_impurity_decrease"
  },
  {
    "objectID": "posts/1-tree.html#other-topics",
    "href": "posts/1-tree.html#other-topics",
    "title": "Decision Tree",
    "section": "Other Topics",
    "text": "Other Topics\n\nPruning\n\n\nMissing Value\n\n\nMonotonic Constraint"
  },
  {
    "objectID": "posts/1-tree.html#the-parameters",
    "href": "posts/1-tree.html#the-parameters",
    "title": "Decision Tree",
    "section": "The Parameters",
    "text": "The Parameters\nDecisionTreeClassifier?\n\ncriterion='gini'\nsplitter='best'\nmax_depth=None\nmin_samples_split=2\nmin_samples_leaf=1\nmin_weight_fraction_leaf=0.0\nmax_features=None\nrandom_state=None\nmax_leaf_nodes=None\nmin_impurity_decrease=0.0\nclass_weight=None\nccp_alpha=0.0\nmonotonic_cst=None"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Random Notes",
    "section": "",
    "text": "Binning With Tree\n\n\n\n\n\n\nmachine learning\n\n\ntree\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nMark Wang\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Tree\n\n\n\n\n\n\nsklearn\n\n\nmachine learning\n\n\ndecision tree\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nMark Wang\n\n\n\n\n\n\nNo matching items"
  }
]